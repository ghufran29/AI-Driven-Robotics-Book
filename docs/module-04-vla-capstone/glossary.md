# Glossary: Module 4 - Vision-Language-Action (VLA)

## A

**Action Plan**: A structured sequence of ROS 2 actions implementing the user's request, generated by the cognitive core.

**API (Application Programming Interface)**: Interface for interacting with external services like OpenAI's Whisper and GPT-4o APIs.

## C

**Cognitive Core**: The component that handles natural language understanding and action planning using LLMs.

**Cognitive Pipeline**: The processing flow that transforms voice input into robot actions through voice recognition, language understanding, vision processing, and action planning.

## D

**Deep Learning**: A subset of machine learning using neural networks with multiple layers, used in the VLA system for vision and language processing.

**Detection Threshold**: Minimum confidence required for object detection to be considered valid.

## F

**Function Calling**: OpenAI's capability to force structured outputs from LLMs, used to generate valid ROS 2 actions.

## G

**Grounding**: The process of connecting linguistic descriptions to specific visual objects in the robot's environment.

**Grounding Pipeline**: The system component that connects language descriptions to visual objects.

## L

**LLM (Large Language Model)**: Advanced AI models like GPT-4o used for natural language understanding and action planning.

**Latency**: The time delay between voice command and robot action execution, targeted to be under 5 seconds.

## M

**Microphone Index**: Numeric identifier for the audio input device used by the voice interface.

**Multi-Modal AI**: AI systems that process multiple types of input simultaneously, such as voice and vision in the VLA system.

## P

**Prompt Engineering**: The practice of crafting effective prompts to guide LLM behavior for specific tasks.

## R

**ROS (Robot Operating System)**: The middleware framework used for robot communication, with actions being the primary interaction method in VLA.

**ROS Action**: A goal-oriented communication pattern in ROS 2 for long-running robot tasks.

## S

**Safety Guardrails**: Validation rules and systems preventing the robot from executing dangerous commands.

**Semantic Analysis**: The process of understanding the meaning and intent behind natural language commands.

**Speech Recognition**: The conversion of spoken language into text, implemented using OpenAI's Whisper API.

## T

**TF-IDF (Term Frequency-Inverse Document Frequency)**: A statistical measure used in text processing to evaluate the importance of words in the grounding pipeline.

**Threshold**: A configurable value that determines when a condition is met, such as voice activity detection or object detection confidence.

## V

**VAD (Voice Activity Detection)**: Technology that detects when a person is speaking versus silence or background noise.

**Vision-Language Model**: AI models that can process both visual and linguistic information simultaneously.

**Vision Grounding**: The process of connecting language descriptions to specific objects in visual scenes.

**VLA (Vision-Language-Action)**: The complete system that transforms vision and language inputs into robot actions.

**VQA (Visual Question Answering)**: A capability of the system to answer questions about visual scenes.

## W

**Whisper**: OpenAI's automatic speech recognition system used for voice-to-text conversion in the VLA system.

**Word Embedding**: Numerical representations of words used in natural language processing to understand semantic relationships.

## Z

**Zero-Shot Learning**: The ability of AI models to perform tasks without specific training on those tasks, demonstrated by the LLM's ability to generate ROS actions from natural language.